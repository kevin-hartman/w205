{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2: Tracking User Activity\n",
    "\n",
    "## W205 Section 4 - Kevin Crook\n",
    "\n",
    "## Prepared by Kevin Hartman - 3/10/2020\n",
    "\n",
    "\n",
    "#### Problem Statement\n",
    "\n",
    "- You work at an ed tech firm. You've created a service that delivers assessments, and now lots of different customers (e.g., Pearson) want to publish their assessments on it. You need to get ready for data scientists who work for these customers to run queries on the data.\n",
    "\n",
    "#### Tasks\n",
    "\n",
    "Prepare the infrastructure to land the data in the form and structure it needs\n",
    "to be to be queried.  You will need to:\n",
    "\n",
    "- Publish and consume messages with Kafka\n",
    "- Use Spark to transform the messages\n",
    "- Use Spark to transform the messages so that you can land them in HDFS\n",
    "\n",
    "#### Deliverables\n",
    " - The `docker-compose.yml` used for spinning up the pipeline\n",
    " - The history of the console (e.g. `history > (user-name)-history.txt` )\n",
    " - A notebook containing the results of sample query executions in Spark\n",
    "   - Include answers to 1 to 3 basic business questions for demonstration\n",
    "   - Also include lines relevant to the pipeline initialization process for reproducibility\n",
    " - Additional files and scripts used for processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform pre-step\n",
    "\n",
    "This notebook is designed to be run from a pyspark container.\n",
    "\n",
    "In order to set up your enviroment, follow the instructions in the [README-Setup](README-Setup.md) markdown file.\n",
    "\n",
    "The setup will perform the following tasks in a shell script:\n",
    "\n",
    "\n",
    "1. Launch the container (performed in `project2-stack.sh`)\n",
    "```\n",
    "docker-compose -f docker-compose.yml up\n",
    "```\n",
    "2. Create the kafka topic for the assessment data (performed in `startup.sh`).\n",
    "```\n",
    "docker-compose exec kafka kafka-topics --create --topic assessments --partitions 1 --replication-factor 1 --if-not-exists --zookeeper zookeeper:32181\n",
    "```\n",
    "3. Establish a symbolic link back to the main directory (so we can access files)\n",
    "```\n",
    "docker-compose exec spark ln -s /w205 w205\n",
    "```\n",
    "4. Download the assessment data (a json file)\n",
    "```\n",
    "mkdir data\n",
    "curl -L -o data/assessment-attempts-20180128-121051-nested.json https://goo.gl/ME6hjp\n",
    "```\n",
    "5. Publish the data onto the kafka topic we created in step 2.\n",
    "```\n",
    "docker-compose exec mids bash -c \"cat /w205/project-2-kevin-hartman/data/assessment-attempts-20180128-121051-nested.json | jq '.[]' -c | kafkacat -P -b kafka:29092 -t assessments\"\n",
    "```\n",
    "6. Launch our Jupyter Notebook through pyspark (so we have access to the pyspark environment in this notebook).\n",
    "```\n",
    "docker-compose exec spark env PYSPARK_DRIVER_PYTHON=jupyter PYSPARK_DRIVER_PYTHON_OPTS='notebook --no-browser --port 8888 --ip 0.0.0.0 --allow-root' pyspark\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Confirm access to environment\n",
    "\n",
    "After following the steps in the README-Setup the environment should be ready to go. We'll confirm by executing a few spark command statements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.28.0.5:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f6c4a575d30>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.28.0.5:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create access to the assessments topic\n",
    "\n",
    "Now we'll set up an RDD that reads from a kafka topic called 'assessments'. The assessments topic was created in the setup script and has records that were published from file containing a list of assessments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_assessments = spark.read.format(\"kafka\").option(\"kafka.bootstrap.servers\", \"kafka:29092\").option(\"subscribe\",\"assessments\").option(\"startingOffsets\", \"earliest\").option(\"endingOffsets\", \"latest\").load() \n",
    "\n",
    "raw_assessments.cache()\n",
    "\n",
    "raw_assessments.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'values' element contains our assessment payload. Let's extract them and create a temp table for easy querying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assessments = raw_assessments.select(raw_assessments.value.cast('string'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from pyspark.sql import Row\n",
    "\n",
    "extracted_assessments = assessments.rdd.map(lambda x: Row(**json.loads(x.value))).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "extracted_assessments.registerTempTable('assessments')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run some queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test it by seeing how many assessments there are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|    3280|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(*) from assessments\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many courses?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|     107|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(*) from (select distinct base_exam_id from assessments)\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many people took the course *Introduction to Python*?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|              course|num_takers|\n",
      "+--------------------+----------+\n",
      "|Introduction to P...|       162|\n",
      "+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select exam_name as course, count(*) as num_takers from assessments where exam_name like 'Introduction to Python' group by exam_name\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the *least* popular course?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|              course|num_takers|\n",
      "+--------------------+----------+\n",
      "|Learning to Visua...|         1|\n",
      "+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select exam_name as course, count(*) as num_takers from assessments group by exam_name order by num_takers asc limit 1\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the *most* popular course?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+\n",
      "|      course|num_takers|\n",
      "+------------+----------+\n",
      "|Learning Git|       394|\n",
      "+------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select exam_name as course, count(*) as num_takers from assessments group by exam_name order by num_takers desc limit 1\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the average score and total number of exams taken (including retries) for the 20 most popular courses?\n",
    "\n",
    "First we'll define a lambda function to compute the correct totals and average scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_lambda_correct_total(x):\n",
    "    \n",
    "    raw_dict = json.loads(x.value)\n",
    "    my_list = []\n",
    "    \n",
    "    if \"sequences\" in raw_dict:\n",
    "        \n",
    "        if \"counts\" in raw_dict[\"sequences\"]:\n",
    "            \n",
    "            if \"correct\" in raw_dict[\"sequences\"][\"counts\"] and \"total\" in raw_dict[\"sequences\"][\"counts\"]:\n",
    "                    \n",
    "                my_dict = {\"keen_id\" : raw_dict[\"keen_id\"],\n",
    "                           \"correct\" : raw_dict[\"sequences\"][\"counts\"][\"correct\"], \n",
    "                           \"total\" : raw_dict[\"sequences\"][\"counts\"][\"total\"]}\n",
    "                my_list.append(Row(**my_dict))\n",
    "    \n",
    "    return my_list\n",
    "\n",
    "my_correct_total = assessments.rdd.flatMap(my_lambda_correct_total).toDF()\n",
    "\n",
    "my_correct_total.registerTempTable('ct')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll execute the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+------------------+\n",
      "|                exam|num_takers|         avg_score|\n",
      "+--------------------+----------+------------------+\n",
      "|        Learning Git|       406| 68.27586206896554|\n",
      "|Introduction to P...|       162| 56.66666666666664|\n",
      "|Intermediate Pyth...|       162| 50.92592592592593|\n",
      "|Introduction to J...|       158| 87.59493670886073|\n",
      "|Beginning C# Prog...|       131|56.297709923664115|\n",
      "|Learning to Progr...|       128| 54.46428571428572|\n",
      "|Introduction to M...|       119| 68.69747899159664|\n",
      "|Software Architec...|       109|47.935779816513765|\n",
      "|    Learning Eclipse|        85| 70.58823529411762|\n",
      "|Introduction to B...|        81| 64.50617283950618|\n",
      "|Learning Apache M...|        80|           60.9375|\n",
      "|Beginning Program...|        79| 76.58227848101265|\n",
      "|       Mastering Git|        77| 58.76623376623377|\n",
      "|Advanced Machine ...|        67| 72.38805970149254|\n",
      "|Learning Linux Sy...|        59| 55.50847457627118|\n",
      "|JavaScript: The G...|        58| 65.51724137931035|\n",
      "|        Learning SQL|        57| 73.68421052631578|\n",
      "|Intermediate C# P...|        55|60.909090909090914|\n",
      "|Learning C# Best ...|        53|46.415094339622634|\n",
      "|Practical Java Pr...|        53| 59.43396226415094|\n",
      "+--------------------+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select a.exam_name as exam, \" +\n",
    "          \"count(ct.correct) as num_takers, \" +\n",
    "          \"avg(ct.correct / ct.total)*100 as avg_score \" +\n",
    "          \"from assessments a join ct on a.keen_id = ct.keen_id \" +\n",
    "          \"group by a.exam_name \" +\n",
    "          \"order by num_takers desc \" +\n",
    "          \"limit 20\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Save our extract data\n",
    "\n",
    "Finally we'll save our extracted data to a parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "extracted_assessments.write.parquet(\"/tmp/extracted_assessments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To confirm the file is saved, execute the following command from your console\n",
    "\n",
    "`docker-compose exec cloudera hadoop fs -ls /tmp/extracted_assessments`\n",
    "\n",
    "The results should look something like this:\n",
    "\n",
    "`Found 2 items\n",
    "-rw-r--r--   1 root supergroup          0 2020-03-10 02:42 /tmp/extracted_assessments/_SUCCESS\n",
    "-rw-r--r--   1 root supergroup     345388 2020-03-10 02:42 /tmp/extracted_assessments/part-00000-b996162c-e9d8-4393-a0e4-2c8045181937-c000.snappy.parquet`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Close the notebook and shutdown the containers\n",
    "\n",
    "To exit this report, kindly halt the kernal on this notebook and follow to the container shutdown instructions on the [README-Setup](README-Setup.md) page.\n",
    "\n",
    "For convenience, the script to shutdown the containers is here.\n",
    "\n",
    "```\n",
    "./bin/shutdown.sh\n",
    "```\n",
    "\n",
    "Which will perform the command\n",
    "\n",
    "```\n",
    "docker-compose -f docker-compose.yml down\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
